{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89ac347-2a66-47ae-bf01-88068e094271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import __main__\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c426c36-b106-4434-ad91-b69ebc61e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load news and behavior data\n",
    "news_columns = [\"News ID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\"]\n",
    "news_df = pd.read_csv('./MINDsmall_train/news.tsv', sep='\\t', names=news_columns, header=0).fillna('')\n",
    "news_df['text'] = news_df['Title'] + \" [SEP] \" + news_df['Abstract']  # Combine title and abstract\n",
    "\n",
    "behaviors_df = pd.read_csv('./MINDsmall_train/behaviors.tsv', sep='\\t', \n",
    "                          names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"], header=0)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LEN = 64\n",
    "\n",
    "def encode_text(text):\n",
    "    return tokenizer.encode(text, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
    "\n",
    "news_df['tokens'] = news_df['text'].apply(encode_text)\n",
    "news_dict = dict(zip(news_df['News ID'], news_df['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d1ea5d-18bf-4b51-9a52-6e65966877e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>URL</th>\n",
       "      <th>Title Entities</th>\n",
       "      <th>Abstract Entities</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>50 Worst Habits For Belly Fat [SEP] These seem...</td>\n",
       "      <td>[101, 2753, 5409, 14243, 2005, 7579, 6638, 102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>[101, 1996, 3465, 1997, 8398, 1005, 1055, 4681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>[101, 1045, 2001, 2019, 6452, 2564, 1012, 2182...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News ID Category SubCategory  \\\n",
       "0  N19639   health  weightloss   \n",
       "1  N61837     news   newsworld   \n",
       "2  N53526   health      voices   \n",
       "\n",
       "                                               Title  \\\n",
       "0                      50 Worst Habits For Belly Fat   \n",
       "1  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "2  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  These seemingly harmless habits are holding yo...   \n",
       "1  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "2  I felt like I was a fraud, and being an NBA wi...   \n",
       "\n",
       "                                             URL  \\\n",
       "0  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "1  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "2  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "\n",
       "                                      Title Entities  \\\n",
       "0  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "\n",
       "                                   Abstract Entities  \\\n",
       "0  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "1  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...   \n",
       "2  [{\"Label\": \"National Basketball Association\", ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  50 Worst Habits For Belly Fat [SEP] These seem...   \n",
       "1  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "2  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [101, 2753, 5409, 14243, 2005, 7579, 6638, 102...  \n",
       "1  [101, 1996, 3465, 1997, 8398, 1005, 1055, 4681...  \n",
       "2  [101, 1045, 2001, 2019, 6452, 2564, 1012, 2182...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d826907-6721-4989-ba98-0339797b3877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Impression ID</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>History</th>\n",
       "      <th>Impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>U91836</td>\n",
       "      <td>11/12/2019 6:11:30 PM</td>\n",
       "      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>U73700</td>\n",
       "      <td>11/14/2019 7:01:48 AM</td>\n",
       "      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n",
       "      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>U34670</td>\n",
       "      <td>11/11/2019 5:28:05 AM</td>\n",
       "      <td>N45729 N2203 N871 N53880 N41375 N43142 N33013 ...</td>\n",
       "      <td>N35729-0 N33632-0 N49685-1 N27581-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Impression ID User ID                   Time  \\\n",
       "0              2  U91836  11/12/2019 6:11:30 PM   \n",
       "1              3  U73700  11/14/2019 7:01:48 AM   \n",
       "2              4  U34670  11/11/2019 5:28:05 AM   \n",
       "\n",
       "                                             History  \\\n",
       "0  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
       "1  N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
       "2  N45729 N2203 N871 N53880 N41375 N43142 N33013 ...   \n",
       "\n",
       "                                         Impressions  \n",
       "0  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n",
       "1  N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  \n",
       "2                N35729-0 N33632-0 N49685-1 N27581-0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c1aa50-6feb-4d3f-bcb1-857fc1514107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "def parse_behaviors(behaviors):\n",
    "    samples = []\n",
    "    for _, row in behaviors.iterrows():\n",
    "        history = row['History'].split() if pd.notna(row['History']) else []\n",
    "        impressions = row['Impressions'].split()\n",
    "        for impression in impressions:\n",
    "            nid, label = impression.split('-')\n",
    "            samples.append((history, nid, int(label)))\n",
    "    return samples\n",
    "\n",
    "samples = parse_behaviors(behaviors_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b62d317-bcd9-425a-9239-b79aeb100870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define NewsDataset in its own cell (or at top of script)\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, samples, news_dict):\n",
    "        self.samples = samples\n",
    "        self.news_dict = news_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        history_ids, candidate_id, label = self.samples[idx]\n",
    "        history_tokens = [self.news_dict[nid] for nid in history_ids if nid in self.news_dict]\n",
    "        candidate_tokens = self.news_dict.get(candidate_id, [0]*MAX_LEN)\n",
    "        \n",
    "        max_history_len = 50\n",
    "        if len(history_tokens) > max_history_len:\n",
    "            history_tokens = history_tokens[-max_history_len:]\n",
    "        else:\n",
    "            history_tokens.extend([[0]*MAX_LEN]*(max_history_len - len(history_tokens)))\n",
    "\n",
    "        return {\n",
    "            'history': torch.tensor(history_tokens),\n",
    "            'candidate': torch.tensor(candidate_tokens),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b1ffb4-63b4-4e96-8a08-907e49aa0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified model\n",
    "class NRMS(nn.Module):\n",
    "    def __init__(self, embedding_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode_news(self, x):\n",
    "        if len(x.shape) == 3:  # History (batch_size, hist_len, seq_len)\n",
    "            batch_size, hist_len, seq_len = x.shape\n",
    "            x = x.view(-1, seq_len)  # (batch_size*hist_len, seq_len)\n",
    "            x = self.embedding(x)  # (batch_size*hist_len, seq_len, embed_dim)\n",
    "            x = x.mean(dim=1)  # (batch_size*hist_len, embed_dim)\n",
    "            x = x.view(batch_size, hist_len, -1)  # (batch_size, hist_len, embed_dim)\n",
    "            x = self.news_encoder(x)  # (batch_size, hist_len, embed_dim)\n",
    "            x = x.mean(dim=1)  # (batch_size, embed_dim)\n",
    "        else:  # Candidate (batch_size, seq_len)\n",
    "            x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "            x = x.mean(dim=1)  # (batch_size, embed_dim)\n",
    "            x = self.news_encoder(x)  # (batch_size, embed_dim)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, history, candidate):\n",
    "        user_vec = self.encode_news(history)  # (batch_size, embed_dim)\n",
    "        news_vec = self.encode_news(candidate)  # (batch_size, embed_dim)\n",
    "        interaction = user_vec * news_vec  # (batch_size, embed_dim)\n",
    "        return self.final_layer(interaction).squeeze()  # (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c4207d-5a21-4807-bf65-9c17b9826b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_mrr(y_true, y_pred):\n",
    "    \"\"\"Calculate MRR using scikit-learn style implementation\"\"\"\n",
    "    # For binary classification\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    # Get ranking positions of positive samples\n",
    "    ranking_positions = []\n",
    "    for i in range(len(y_true)):\n",
    "        if np.sum(y_true[i]) > 0:  # Only calculate for queries with positive samples\n",
    "            pred_rank = np.argsort(-y_pred[i])  # Sort descending\n",
    "            pos_rank = np.where(pred_rank == np.argmax(y_true[i]))[0][0] + 1\n",
    "            ranking_positions.append(1.0 / pos_rank)\n",
    "    \n",
    "    return np.mean(ranking_positions) if ranking_positions else 0.0\n",
    "\n",
    "def calculate_ndcg(y_true, y_pred, k):\n",
    "    \"\"\"Calculate NDCG@k using scikit-learn style implementation\"\"\"\n",
    "    # For binary classification\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        # Sort predictions and get top k\n",
    "        order = np.argsort(-y_pred[i])[:k]\n",
    "        # Calculate DCG\n",
    "        dcg = np.sum(y_true[i][order] / np.log2(np.arange(2, k+2)))\n",
    "        # Calculate IDCG\n",
    "        ideal_order = np.argsort(-y_true[i])[:k]\n",
    "        idcg = np.sum(y_true[i][ideal_order] / np.log2(np.arange(2, k+2)))\n",
    "        ndcg_scores.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "    \n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            outputs = model(batch['history'], batch['candidate'])\n",
    "            all_labels.append(batch['label'].cpu().numpy())\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "    \n",
    "    y_true = np.concatenate(all_labels)\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    \n",
    "    # Handle binary classification case\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    return (\n",
    "        roc_auc_score(y_true, y_pred),\n",
    "        calculate_mrr(y_true, y_pred),\n",
    "        calculate_ndcg(y_true, y_pred, 5),\n",
    "        calculate_ndcg(y_true, y_pred, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4489846-1e53-42a5-af2e-1913816bd27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 236343 out of 5843442\n",
      "Sample tokens: [101, 2662, 10558, 6869, 2000, 1016, 11573, 2273, 2915, 2007, 12563, 1024, 3189, 102, 2048, 11573, 2273, 2542, 2012, 2019, 4372, 26468, 3672, 2379, 2624, 3799, 2020, 2915, 2007, 12563, 2220, 4465, 1999, 2019, 2886, 2008, 2001, 2025, 6721, 1010, 1037, 3189, 2056, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "print(f\"Positive samples: {sum(samples[i][2] for i in range(len(samples)))} out of {len(samples)}\")\n",
    "\n",
    "# Verify tokenization\n",
    "sample_idx = random.randint(0, len(samples)-1)\n",
    "print(f\"Sample tokens: {news_dict[samples[sample_idx][1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01458080-95bd-4ac3-aef3-d74c7dab1c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28520d8-b8ca-49d8-bc50-467536553acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e357dbc5a848d1b98d0408c2349bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a73c6a9710466b95a064980f7dc843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss 0.7283\n",
      "Batch 100: Loss 0.2018\n",
      "Batch 200: Loss 0.1897\n",
      "Batch 300: Loss 0.1862\n",
      "Batch 400: Loss 0.1853\n",
      "Batch 500: Loss 0.1842\n",
      "Batch 600: Loss 0.1803\n",
      "Batch 700: Loss 0.1788\n",
      "Batch 800: Loss 0.1771\n",
      "Batch 900: Loss 0.1762\n",
      "Batch 1000: Loss 0.1747\n",
      "Batch 1100: Loss 0.1741\n",
      "Batch 1200: Loss 0.1728\n",
      "Batch 1300: Loss 0.1735\n",
      "Batch 1400: Loss 0.1734\n",
      "Batch 1500: Loss 0.1733\n",
      "Batch 1600: Loss 0.1718\n",
      "Batch 1700: Loss 0.1703\n",
      "Batch 1800: Loss 0.1709\n",
      "Batch 1900: Loss 0.1706\n",
      "Batch 2000: Loss 0.1704\n",
      "Batch 2100: Loss 0.1701\n",
      "Batch 2200: Loss 0.1696\n",
      "Batch 2300: Loss 0.1691\n",
      "Batch 2400: Loss 0.1688\n",
      "Batch 2500: Loss 0.1688\n",
      "Batch 2600: Loss 0.1683\n",
      "Batch 2700: Loss 0.1678\n",
      "Batch 2800: Loss 0.1673\n",
      "Batch 2900: Loss 0.1674\n",
      "Batch 3000: Loss 0.1673\n",
      "Batch 3100: Loss 0.1672\n",
      "Batch 3200: Loss 0.1665\n",
      "Batch 3300: Loss 0.1666\n",
      "Batch 3400: Loss 0.1660\n",
      "Batch 3500: Loss 0.1663\n",
      "Batch 3600: Loss 0.1664\n",
      "Batch 3700: Loss 0.1659\n",
      "Batch 3800: Loss 0.1663\n",
      "Batch 3900: Loss 0.1665\n",
      "Batch 4000: Loss 0.1668\n",
      "Batch 4100: Loss 0.1669\n",
      "Batch 4200: Loss 0.1670\n",
      "Batch 4300: Loss 0.1670\n",
      "Batch 4400: Loss 0.1666\n",
      "Batch 4500: Loss 0.1666\n",
      "Batch 4600: Loss 0.1664\n",
      "Batch 4700: Loss 0.1664\n",
      "Batch 4800: Loss 0.1669\n",
      "Batch 4900: Loss 0.1670\n",
      "Batch 5000: Loss 0.1667\n",
      "Batch 5100: Loss 0.1665\n",
      "Batch 5200: Loss 0.1665\n",
      "Batch 5300: Loss 0.1665\n",
      "Batch 5400: Loss 0.1664\n",
      "Batch 5500: Loss 0.1666\n",
      "Batch 5600: Loss 0.1665\n",
      "Batch 5700: Loss 0.1664\n",
      "Batch 5800: Loss 0.1661\n",
      "Batch 5900: Loss 0.1663\n",
      "Batch 6000: Loss 0.1664\n",
      "Batch 6100: Loss 0.1662\n",
      "Batch 6200: Loss 0.1659\n",
      "Batch 6300: Loss 0.1656\n",
      "Batch 6400: Loss 0.1655\n",
      "Batch 6500: Loss 0.1656\n",
      "Batch 6600: Loss 0.1658\n",
      "Batch 6700: Loss 0.1658\n",
      "Batch 6800: Loss 0.1655\n",
      "Batch 6900: Loss 0.1653\n",
      "Batch 7000: Loss 0.1652\n",
      "Batch 7100: Loss 0.1650\n",
      "Batch 7200: Loss 0.1649\n",
      "Batch 7300: Loss 0.1647\n",
      "Batch 7400: Loss 0.1645\n",
      "Batch 7500: Loss 0.1644\n",
      "Batch 7600: Loss 0.1646\n",
      "Batch 7700: Loss 0.1646\n",
      "Batch 7800: Loss 0.1646\n",
      "Batch 7900: Loss 0.1645\n",
      "Batch 8000: Loss 0.1645\n",
      "Batch 8100: Loss 0.1646\n",
      "Batch 8200: Loss 0.1646\n",
      "Batch 8300: Loss 0.1645\n",
      "Batch 8400: Loss 0.1643\n",
      "Batch 8500: Loss 0.1643\n",
      "Batch 8600: Loss 0.1640\n",
      "Batch 8700: Loss 0.1638\n",
      "Batch 8800: Loss 0.1635\n",
      "Batch 8900: Loss 0.1634\n",
      "Batch 9000: Loss 0.1634\n",
      "Batch 9100: Loss 0.1634\n",
      "Batch 9200: Loss 0.1636\n",
      "Batch 9300: Loss 0.1635\n",
      "Batch 9400: Loss 0.1632\n",
      "Batch 9500: Loss 0.1631\n",
      "Batch 9600: Loss 0.1632\n",
      "Batch 9700: Loss 0.1632\n",
      "Batch 9800: Loss 0.1634\n",
      "Batch 9900: Loss 0.1633\n",
      "Batch 10000: Loss 0.1632\n",
      "Batch 10100: Loss 0.1632\n",
      "Batch 10200: Loss 0.1632\n",
      "Batch 10300: Loss 0.1632\n",
      "Batch 10400: Loss 0.1631\n",
      "Batch 10500: Loss 0.1630\n",
      "Batch 10600: Loss 0.1630\n",
      "Batch 10700: Loss 0.1628\n",
      "Batch 10800: Loss 0.1626\n",
      "Batch 10900: Loss 0.1625\n",
      "Batch 11000: Loss 0.1625\n",
      "Batch 11100: Loss 0.1625\n",
      "Batch 11200: Loss 0.1624\n",
      "Batch 11300: Loss 0.1623\n",
      "Batch 11400: Loss 0.1622\n",
      "Batch 11500: Loss 0.1621\n",
      "Batch 11600: Loss 0.1622\n",
      "Batch 11700: Loss 0.1621\n",
      "Batch 11800: Loss 0.1623\n",
      "Batch 11900: Loss 0.1622\n",
      "Batch 12000: Loss 0.1622\n",
      "Batch 12100: Loss 0.1620\n",
      "Batch 12200: Loss 0.1621\n",
      "Batch 12300: Loss 0.1621\n",
      "Batch 12400: Loss 0.1621\n",
      "Batch 12500: Loss 0.1623\n",
      "Batch 12600: Loss 0.1623\n",
      "Batch 12700: Loss 0.1625\n",
      "Batch 12800: Loss 0.1624\n",
      "Batch 12900: Loss 0.1625\n",
      "Batch 13000: Loss 0.1625\n",
      "Batch 13100: Loss 0.1624\n",
      "Batch 13200: Loss 0.1624\n",
      "Batch 13300: Loss 0.1625\n",
      "Batch 13400: Loss 0.1625\n",
      "Batch 13500: Loss 0.1625\n",
      "Batch 13600: Loss 0.1625\n",
      "Batch 13700: Loss 0.1626\n",
      "Batch 13800: Loss 0.1625\n",
      "Batch 13900: Loss 0.1628\n",
      "Batch 14000: Loss 0.1627\n",
      "Batch 14100: Loss 0.1627\n",
      "Batch 14200: Loss 0.1628\n",
      "Batch 14300: Loss 0.1627\n",
      "Batch 14400: Loss 0.1626\n",
      "Batch 14500: Loss 0.1627\n",
      "Batch 14600: Loss 0.1626\n",
      "Batch 14700: Loss 0.1626\n",
      "Batch 14800: Loss 0.1626\n",
      "Batch 14900: Loss 0.1626\n",
      "Batch 15000: Loss 0.1625\n",
      "Batch 15100: Loss 0.1624\n",
      "Batch 15200: Loss 0.1624\n",
      "Batch 15300: Loss 0.1622\n",
      "Batch 15400: Loss 0.1623\n",
      "Batch 15500: Loss 0.1622\n",
      "Batch 15600: Loss 0.1624\n",
      "Batch 15700: Loss 0.1622\n",
      "Batch 15800: Loss 0.1621\n",
      "Batch 15900: Loss 0.1622\n",
      "Batch 16000: Loss 0.1621\n",
      "Batch 16100: Loss 0.1620\n",
      "Batch 16200: Loss 0.1620\n",
      "Batch 16300: Loss 0.1620\n",
      "Batch 16400: Loss 0.1619\n",
      "Batch 16500: Loss 0.1620\n",
      "Batch 16600: Loss 0.1619\n",
      "Batch 16700: Loss 0.1620\n",
      "Batch 16800: Loss 0.1620\n",
      "Batch 16900: Loss 0.1620\n",
      "Batch 17000: Loss 0.1620\n",
      "Batch 17100: Loss 0.1619\n",
      "Batch 17200: Loss 0.1619\n",
      "Batch 17300: Loss 0.1619\n",
      "Batch 17400: Loss 0.1619\n",
      "Batch 17500: Loss 0.1619\n",
      "Batch 17600: Loss 0.1618\n",
      "Batch 17700: Loss 0.1618\n",
      "Batch 17800: Loss 0.1618\n",
      "Batch 17900: Loss 0.1618\n",
      "Batch 18000: Loss 0.1618\n",
      "Batch 18100: Loss 0.1617\n",
      "Batch 18200: Loss 0.1617\n",
      "Batch 18300: Loss 0.1616\n",
      "Batch 18400: Loss 0.1616\n",
      "Batch 18500: Loss 0.1616\n",
      "Batch 18600: Loss 0.1615\n",
      "Batch 18700: Loss 0.1615\n",
      "Batch 18800: Loss 0.1614\n",
      "Batch 18900: Loss 0.1613\n",
      "Batch 19000: Loss 0.1613\n",
      "Batch 19100: Loss 0.1612\n",
      "Batch 19200: Loss 0.1612\n",
      "Batch 19300: Loss 0.1612\n",
      "Batch 19400: Loss 0.1611\n",
      "Batch 19500: Loss 0.1611\n",
      "Batch 19600: Loss 0.1611\n",
      "Batch 19700: Loss 0.1610\n",
      "Batch 19800: Loss 0.1611\n",
      "Batch 19900: Loss 0.1611\n",
      "Batch 20000: Loss 0.1611\n",
      "Batch 20100: Loss 0.1611\n",
      "Batch 20200: Loss 0.1611\n",
      "Batch 20300: Loss 0.1612\n",
      "Batch 20400: Loss 0.1610\n",
      "Batch 20500: Loss 0.1610\n",
      "Batch 20600: Loss 0.1609\n",
      "Batch 20700: Loss 0.1609\n",
      "Batch 20800: Loss 0.1609\n",
      "Batch 20900: Loss 0.1609\n",
      "Batch 21000: Loss 0.1609\n",
      "Batch 21100: Loss 0.1609\n",
      "Batch 21200: Loss 0.1609\n",
      "Batch 21300: Loss 0.1608\n",
      "Batch 21400: Loss 0.1609\n",
      "Batch 21500: Loss 0.1609\n",
      "Batch 21600: Loss 0.1609\n",
      "Batch 21700: Loss 0.1609\n",
      "Batch 21800: Loss 0.1608\n",
      "Batch 21900: Loss 0.1607\n",
      "Batch 22000: Loss 0.1607\n",
      "Batch 22100: Loss 0.1608\n",
      "Batch 22200: Loss 0.1608\n",
      "Batch 22300: Loss 0.1608\n",
      "Batch 22400: Loss 0.1608\n",
      "Batch 22500: Loss 0.1608\n",
      "Batch 22600: Loss 0.1608\n",
      "Batch 22700: Loss 0.1608\n",
      "Batch 22800: Loss 0.1607\n",
      "Batch 22900: Loss 0.1605\n",
      "Batch 23000: Loss 0.1605\n",
      "Batch 23100: Loss 0.1604\n",
      "Batch 23200: Loss 0.1605\n",
      "Batch 23300: Loss 0.1604\n",
      "Batch 23400: Loss 0.1603\n",
      "Batch 23500: Loss 0.1603\n",
      "Batch 23600: Loss 0.1603\n",
      "Batch 23700: Loss 0.1603\n",
      "Batch 23800: Loss 0.1603\n",
      "Batch 23900: Loss 0.1603\n",
      "Batch 24000: Loss 0.1603\n",
      "Batch 24100: Loss 0.1604\n",
      "Batch 24200: Loss 0.1604\n",
      "Batch 24300: Loss 0.1604\n",
      "Batch 24400: Loss 0.1604\n",
      "Batch 24500: Loss 0.1604\n",
      "Batch 24600: Loss 0.1604\n",
      "Batch 24700: Loss 0.1603\n",
      "Batch 24800: Loss 0.1603\n",
      "Batch 24900: Loss 0.1602\n",
      "Batch 25000: Loss 0.1602\n",
      "Batch 25100: Loss 0.1602\n",
      "Batch 25200: Loss 0.1601\n",
      "Batch 25300: Loss 0.1601\n",
      "Batch 25400: Loss 0.1601\n",
      "Batch 25500: Loss 0.1602\n",
      "Batch 25600: Loss 0.1602\n",
      "Batch 25700: Loss 0.1602\n",
      "Batch 25800: Loss 0.1601\n",
      "Batch 25900: Loss 0.1601\n",
      "Batch 26000: Loss 0.1601\n",
      "Batch 26100: Loss 0.1600\n",
      "Batch 26200: Loss 0.1600\n",
      "Batch 26300: Loss 0.1600\n",
      "Batch 26400: Loss 0.1600\n",
      "Batch 26500: Loss 0.1600\n",
      "Batch 26600: Loss 0.1600\n",
      "Batch 26700: Loss 0.1600\n",
      "Batch 26800: Loss 0.1600\n",
      "Batch 26900: Loss 0.1600\n",
      "Batch 27000: Loss 0.1599\n",
      "Batch 27100: Loss 0.1599\n",
      "Batch 27200: Loss 0.1599\n",
      "Batch 27300: Loss 0.1599\n",
      "Batch 27400: Loss 0.1599\n",
      "Batch 27500: Loss 0.1599\n",
      "Batch 27600: Loss 0.1599\n",
      "Batch 27700: Loss 0.1599\n",
      "Batch 27800: Loss 0.1599\n",
      "Batch 27900: Loss 0.1599\n",
      "Batch 28000: Loss 0.1600\n",
      "Batch 28100: Loss 0.1600\n",
      "Batch 28200: Loss 0.1599\n",
      "Batch 28300: Loss 0.1599\n",
      "Batch 28400: Loss 0.1599\n",
      "Batch 28500: Loss 0.1599\n",
      "Batch 28600: Loss 0.1599\n",
      "Batch 28700: Loss 0.1599\n",
      "Batch 28800: Loss 0.1600\n",
      "Batch 28900: Loss 0.1600\n",
      "Batch 29000: Loss 0.1600\n",
      "Batch 29100: Loss 0.1599\n",
      "Batch 29200: Loss 0.1600\n",
      "Batch 29300: Loss 0.1600\n",
      "Batch 29400: Loss 0.1600\n",
      "Batch 29500: Loss 0.1600\n",
      "Batch 29600: Loss 0.1601\n",
      "Batch 29700: Loss 0.1600\n",
      "Batch 29800: Loss 0.1600\n",
      "Batch 29900: Loss 0.1600\n",
      "Batch 30000: Loss 0.1600\n",
      "Batch 30100: Loss 0.1600\n",
      "Batch 30200: Loss 0.1600\n",
      "Batch 30300: Loss 0.1600\n",
      "Batch 30400: Loss 0.1600\n",
      "Batch 30500: Loss 0.1601\n",
      "Batch 30600: Loss 0.1600\n",
      "Batch 30700: Loss 0.1600\n",
      "Batch 30800: Loss 0.1600\n",
      "Batch 30900: Loss 0.1600\n",
      "Batch 31000: Loss 0.1600\n",
      "Batch 31100: Loss 0.1600\n",
      "Batch 31200: Loss 0.1600\n",
      "Batch 31300: Loss 0.1600\n",
      "Batch 31400: Loss 0.1600\n",
      "Batch 31500: Loss 0.1599\n",
      "Batch 31600: Loss 0.1599\n",
      "Batch 31700: Loss 0.1599\n",
      "Batch 31800: Loss 0.1599\n",
      "Batch 31900: Loss 0.1599\n",
      "Batch 32000: Loss 0.1599\n",
      "Batch 32100: Loss 0.1599\n",
      "Batch 32200: Loss 0.1599\n",
      "Batch 32300: Loss 0.1599\n",
      "Batch 32400: Loss 0.1599\n",
      "Batch 32500: Loss 0.1599\n",
      "Batch 32600: Loss 0.1599\n",
      "Batch 32700: Loss 0.1598\n",
      "Batch 32800: Loss 0.1599\n",
      "Batch 32900: Loss 0.1599\n",
      "Batch 33000: Loss 0.1598\n",
      "Batch 33100: Loss 0.1598\n",
      "Batch 33200: Loss 0.1598\n",
      "Batch 33300: Loss 0.1597\n",
      "Batch 33400: Loss 0.1598\n",
      "Batch 33500: Loss 0.1598\n",
      "Batch 33600: Loss 0.1597\n",
      "Batch 33700: Loss 0.1597\n",
      "Batch 33800: Loss 0.1598\n",
      "Batch 33900: Loss 0.1597\n",
      "Batch 34000: Loss 0.1597\n",
      "Batch 34100: Loss 0.1597\n",
      "Batch 34200: Loss 0.1597\n",
      "Batch 34300: Loss 0.1597\n",
      "Batch 34400: Loss 0.1597\n",
      "Batch 34500: Loss 0.1597\n",
      "Batch 34600: Loss 0.1597\n",
      "Batch 34700: Loss 0.1597\n",
      "Batch 34800: Loss 0.1597\n",
      "Batch 34900: Loss 0.1598\n",
      "Batch 35000: Loss 0.1597\n",
      "Batch 35100: Loss 0.1597\n",
      "Batch 35200: Loss 0.1597\n",
      "Batch 35300: Loss 0.1598\n",
      "Batch 35400: Loss 0.1597\n",
      "Batch 35500: Loss 0.1598\n",
      "Batch 35600: Loss 0.1597\n",
      "Batch 35700: Loss 0.1597\n",
      "Batch 35800: Loss 0.1597\n",
      "Batch 35900: Loss 0.1597\n",
      "Batch 36000: Loss 0.1597\n",
      "Batch 36100: Loss 0.1597\n",
      "Batch 36200: Loss 0.1596\n",
      "Batch 36300: Loss 0.1596\n",
      "Batch 36400: Loss 0.1596\n",
      "Batch 36500: Loss 0.1596\n",
      "\n",
      "Epoch 1 Metrics:\n",
      "AUC: 0.7282 | MRR: 1.0000 | NDCG@5: 0.0406 | NDCG@10: 0.0406\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3161648acf475f9a8a08201fe878f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss 0.2341\n",
      "Batch 100: Loss 0.1655\n",
      "Batch 200: Loss 0.1658\n",
      "Batch 300: Loss 0.1667\n",
      "Batch 400: Loss 0.1636\n",
      "Batch 500: Loss 0.1623\n",
      "Batch 600: Loss 0.1624\n",
      "Batch 700: Loss 0.1604\n",
      "Batch 800: Loss 0.1579\n",
      "Batch 900: Loss 0.1580\n",
      "Batch 1000: Loss 0.1570\n",
      "Batch 1100: Loss 0.1569\n",
      "Batch 1200: Loss 0.1573\n",
      "Batch 1300: Loss 0.1572\n",
      "Batch 1400: Loss 0.1569\n",
      "Batch 1500: Loss 0.1566\n",
      "Batch 1600: Loss 0.1567\n",
      "Batch 1700: Loss 0.1567\n",
      "Batch 1800: Loss 0.1572\n",
      "Batch 1900: Loss 0.1575\n",
      "Batch 2000: Loss 0.1575\n",
      "Batch 2100: Loss 0.1571\n",
      "Batch 2200: Loss 0.1572\n",
      "Batch 2300: Loss 0.1574\n",
      "Batch 2400: Loss 0.1575\n",
      "Batch 2500: Loss 0.1577\n",
      "Batch 2600: Loss 0.1578\n",
      "Batch 2700: Loss 0.1568\n",
      "Batch 2800: Loss 0.1562\n",
      "Batch 2900: Loss 0.1563\n",
      "Batch 3000: Loss 0.1557\n",
      "Batch 3100: Loss 0.1555\n",
      "Batch 3200: Loss 0.1559\n",
      "Batch 3300: Loss 0.1565\n",
      "Batch 3400: Loss 0.1568\n",
      "Batch 3500: Loss 0.1569\n",
      "Batch 3600: Loss 0.1568\n",
      "Batch 3700: Loss 0.1567\n",
      "Batch 3800: Loss 0.1566\n",
      "Batch 3900: Loss 0.1562\n",
      "Batch 4000: Loss 0.1564\n",
      "Batch 4100: Loss 0.1569\n",
      "Batch 4200: Loss 0.1568\n",
      "Batch 4300: Loss 0.1566\n",
      "Batch 4400: Loss 0.1567\n",
      "Batch 4500: Loss 0.1566\n",
      "Batch 4600: Loss 0.1565\n",
      "Batch 4700: Loss 0.1566\n",
      "Batch 4800: Loss 0.1562\n",
      "Batch 4900: Loss 0.1564\n",
      "Batch 5000: Loss 0.1561\n",
      "Batch 5100: Loss 0.1563\n",
      "Batch 5200: Loss 0.1565\n",
      "Batch 5300: Loss 0.1563\n",
      "Batch 5400: Loss 0.1566\n",
      "Batch 5500: Loss 0.1565\n",
      "Batch 5600: Loss 0.1561\n",
      "Batch 5700: Loss 0.1562\n",
      "Batch 5800: Loss 0.1559\n",
      "Batch 5900: Loss 0.1558\n",
      "Batch 6000: Loss 0.1559\n",
      "Batch 6100: Loss 0.1561\n",
      "Batch 6200: Loss 0.1565\n",
      "Batch 6300: Loss 0.1566\n",
      "Batch 6400: Loss 0.1565\n",
      "Batch 6500: Loss 0.1565\n",
      "Batch 6600: Loss 0.1566\n",
      "Batch 6700: Loss 0.1563\n",
      "Batch 6800: Loss 0.1562\n",
      "Batch 6900: Loss 0.1562\n",
      "Batch 7000: Loss 0.1564\n",
      "Batch 7100: Loss 0.1563\n",
      "Batch 7200: Loss 0.1564\n",
      "Batch 7300: Loss 0.1561\n",
      "Batch 7400: Loss 0.1560\n",
      "Batch 7500: Loss 0.1560\n",
      "Batch 7600: Loss 0.1557\n",
      "Batch 7700: Loss 0.1557\n",
      "Batch 7800: Loss 0.1558\n",
      "Batch 7900: Loss 0.1558\n",
      "Batch 8000: Loss 0.1558\n",
      "Batch 8100: Loss 0.1557\n",
      "Batch 8200: Loss 0.1556\n",
      "Batch 8300: Loss 0.1555\n",
      "Batch 8400: Loss 0.1557\n",
      "Batch 8500: Loss 0.1556\n",
      "Batch 8600: Loss 0.1557\n",
      "Batch 8700: Loss 0.1555\n",
      "Batch 8800: Loss 0.1555\n",
      "Batch 8900: Loss 0.1555\n",
      "Batch 9000: Loss 0.1552\n",
      "Batch 9100: Loss 0.1551\n",
      "Batch 9200: Loss 0.1554\n",
      "Batch 9300: Loss 0.1555\n",
      "Batch 9400: Loss 0.1555\n",
      "Batch 9500: Loss 0.1556\n",
      "Batch 9600: Loss 0.1555\n",
      "Batch 9700: Loss 0.1556\n",
      "Batch 9800: Loss 0.1555\n",
      "Batch 9900: Loss 0.1554\n",
      "Batch 10000: Loss 0.1554\n",
      "Batch 10100: Loss 0.1553\n",
      "Batch 10200: Loss 0.1552\n",
      "Batch 10300: Loss 0.1553\n",
      "Batch 10400: Loss 0.1552\n",
      "Batch 10500: Loss 0.1553\n",
      "Batch 10600: Loss 0.1555\n",
      "Batch 10700: Loss 0.1553\n",
      "Batch 10800: Loss 0.1554\n",
      "Batch 10900: Loss 0.1554\n",
      "Batch 11000: Loss 0.1556\n",
      "Batch 11100: Loss 0.1557\n",
      "Batch 11200: Loss 0.1557\n",
      "Batch 11300: Loss 0.1556\n",
      "Batch 11400: Loss 0.1557\n",
      "Batch 11500: Loss 0.1556\n",
      "Batch 11600: Loss 0.1554\n",
      "Batch 11700: Loss 0.1553\n",
      "Batch 11800: Loss 0.1554\n",
      "Batch 11900: Loss 0.1555\n",
      "Batch 12000: Loss 0.1555\n",
      "Batch 12100: Loss 0.1554\n",
      "Batch 12200: Loss 0.1555\n",
      "Batch 12300: Loss 0.1555\n",
      "Batch 12400: Loss 0.1556\n",
      "Batch 12500: Loss 0.1556\n",
      "Batch 12600: Loss 0.1558\n",
      "Batch 12700: Loss 0.1557\n",
      "Batch 12800: Loss 0.1557\n",
      "Batch 12900: Loss 0.1557\n",
      "Batch 13000: Loss 0.1557\n",
      "Batch 13100: Loss 0.1558\n",
      "Batch 13200: Loss 0.1555\n",
      "Batch 13300: Loss 0.1555\n",
      "Batch 13400: Loss 0.1554\n",
      "Batch 13500: Loss 0.1554\n",
      "Batch 13600: Loss 0.1555\n",
      "Batch 13700: Loss 0.1554\n",
      "Batch 13800: Loss 0.1555\n",
      "Batch 13900: Loss 0.1555\n",
      "Batch 14000: Loss 0.1554\n",
      "Batch 14100: Loss 0.1553\n",
      "Batch 14200: Loss 0.1552\n",
      "Batch 14300: Loss 0.1552\n",
      "Batch 14400: Loss 0.1552\n",
      "Batch 14500: Loss 0.1552\n",
      "Batch 14600: Loss 0.1552\n",
      "Batch 14700: Loss 0.1552\n",
      "Batch 14800: Loss 0.1551\n",
      "Batch 14900: Loss 0.1551\n",
      "Batch 15000: Loss 0.1552\n",
      "Batch 15100: Loss 0.1552\n",
      "Batch 15200: Loss 0.1552\n",
      "Batch 15300: Loss 0.1552\n",
      "Batch 15400: Loss 0.1552\n",
      "Batch 15500: Loss 0.1550\n",
      "Batch 15600: Loss 0.1550\n",
      "Batch 15700: Loss 0.1549\n",
      "Batch 15800: Loss 0.1549\n",
      "Batch 15900: Loss 0.1550\n",
      "Batch 16000: Loss 0.1549\n",
      "Batch 16100: Loss 0.1549\n",
      "Batch 16200: Loss 0.1549\n",
      "Batch 16300: Loss 0.1548\n",
      "Batch 16400: Loss 0.1548\n",
      "Batch 16500: Loss 0.1548\n",
      "Batch 16600: Loss 0.1548\n",
      "Batch 16700: Loss 0.1549\n",
      "Batch 16800: Loss 0.1549\n",
      "Batch 16900: Loss 0.1549\n",
      "Batch 17000: Loss 0.1548\n",
      "Batch 17100: Loss 0.1549\n",
      "Batch 17200: Loss 0.1548\n",
      "Batch 17300: Loss 0.1548\n",
      "Batch 17400: Loss 0.1549\n",
      "Batch 17500: Loss 0.1548\n",
      "Batch 17600: Loss 0.1548\n",
      "Batch 17700: Loss 0.1547\n",
      "Batch 17800: Loss 0.1547\n",
      "Batch 17900: Loss 0.1548\n",
      "Batch 18000: Loss 0.1547\n",
      "Batch 18100: Loss 0.1546\n",
      "Batch 18200: Loss 0.1546\n",
      "Batch 18300: Loss 0.1545\n",
      "Batch 18400: Loss 0.1545\n",
      "Batch 18500: Loss 0.1545\n",
      "Batch 18600: Loss 0.1545\n",
      "Batch 18700: Loss 0.1545\n",
      "Batch 18800: Loss 0.1545\n",
      "Batch 18900: Loss 0.1546\n",
      "Batch 19000: Loss 0.1545\n",
      "Batch 19100: Loss 0.1545\n",
      "Batch 19200: Loss 0.1546\n",
      "Batch 19300: Loss 0.1546\n",
      "Batch 19400: Loss 0.1546\n",
      "Batch 19500: Loss 0.1546\n",
      "Batch 19600: Loss 0.1547\n",
      "Batch 19700: Loss 0.1547\n",
      "Batch 19800: Loss 0.1547\n",
      "Batch 19900: Loss 0.1547\n",
      "Batch 20000: Loss 0.1548\n",
      "Batch 20100: Loss 0.1547\n",
      "Batch 20200: Loss 0.1548\n",
      "Batch 20300: Loss 0.1547\n",
      "Batch 20400: Loss 0.1547\n",
      "Batch 20500: Loss 0.1547\n",
      "Batch 20600: Loss 0.1546\n",
      "Batch 20700: Loss 0.1546\n",
      "Batch 20800: Loss 0.1547\n",
      "Batch 20900: Loss 0.1547\n",
      "Batch 21000: Loss 0.1547\n",
      "Batch 21100: Loss 0.1546\n",
      "Batch 21200: Loss 0.1546\n",
      "Batch 21300: Loss 0.1546\n",
      "Batch 21400: Loss 0.1546\n",
      "Batch 21500: Loss 0.1546\n",
      "Batch 21600: Loss 0.1546\n",
      "Batch 21700: Loss 0.1546\n",
      "Batch 21800: Loss 0.1546\n",
      "Batch 21900: Loss 0.1546\n",
      "Batch 22000: Loss 0.1547\n",
      "Batch 22100: Loss 0.1547\n",
      "Batch 22200: Loss 0.1547\n",
      "Batch 22300: Loss 0.1547\n",
      "Batch 22400: Loss 0.1548\n",
      "Batch 22500: Loss 0.1547\n",
      "Batch 22600: Loss 0.1547\n",
      "Batch 22700: Loss 0.1547\n",
      "Batch 22800: Loss 0.1547\n",
      "Batch 22900: Loss 0.1547\n",
      "Batch 23000: Loss 0.1547\n",
      "Batch 23100: Loss 0.1547\n",
      "Batch 23200: Loss 0.1548\n",
      "Batch 23300: Loss 0.1548\n",
      "Batch 23400: Loss 0.1548\n",
      "Batch 23500: Loss 0.1548\n",
      "Batch 23600: Loss 0.1548\n",
      "Batch 23700: Loss 0.1548\n",
      "Batch 23800: Loss 0.1548\n",
      "Batch 23900: Loss 0.1548\n",
      "Batch 24000: Loss 0.1548\n",
      "Batch 24100: Loss 0.1548\n",
      "Batch 24200: Loss 0.1549\n",
      "Batch 24300: Loss 0.1549\n",
      "Batch 24400: Loss 0.1550\n",
      "Batch 24500: Loss 0.1549\n",
      "Batch 24600: Loss 0.1549\n",
      "Batch 24700: Loss 0.1550\n",
      "Batch 24800: Loss 0.1549\n",
      "Batch 24900: Loss 0.1550\n",
      "Batch 25000: Loss 0.1550\n",
      "Batch 25100: Loss 0.1550\n",
      "Batch 25200: Loss 0.1550\n",
      "Batch 25300: Loss 0.1550\n",
      "Batch 25400: Loss 0.1550\n",
      "Batch 25500: Loss 0.1549\n",
      "Batch 25600: Loss 0.1549\n",
      "Batch 25700: Loss 0.1549\n",
      "Batch 25800: Loss 0.1549\n",
      "Batch 25900: Loss 0.1549\n",
      "Batch 26000: Loss 0.1549\n",
      "Batch 26100: Loss 0.1550\n",
      "Batch 26200: Loss 0.1550\n",
      "Batch 26300: Loss 0.1550\n",
      "Batch 26400: Loss 0.1550\n",
      "Batch 26500: Loss 0.1550\n",
      "Batch 26600: Loss 0.1550\n",
      "Batch 26700: Loss 0.1550\n",
      "Batch 26800: Loss 0.1550\n",
      "Batch 26900: Loss 0.1550\n",
      "Batch 27000: Loss 0.1550\n",
      "Batch 27100: Loss 0.1550\n",
      "Batch 27200: Loss 0.1550\n",
      "Batch 27300: Loss 0.1551\n",
      "Batch 27400: Loss 0.1551\n",
      "Batch 27500: Loss 0.1551\n",
      "Batch 27600: Loss 0.1551\n",
      "Batch 27700: Loss 0.1551\n",
      "Batch 27800: Loss 0.1551\n",
      "Batch 27900: Loss 0.1551\n",
      "Batch 28000: Loss 0.1551\n",
      "Batch 28100: Loss 0.1550\n",
      "Batch 28200: Loss 0.1550\n",
      "Batch 28300: Loss 0.1550\n",
      "Batch 28400: Loss 0.1550\n",
      "Batch 28500: Loss 0.1550\n",
      "Batch 28600: Loss 0.1550\n",
      "Batch 28700: Loss 0.1550\n",
      "Batch 28800: Loss 0.1551\n",
      "Batch 28900: Loss 0.1551\n",
      "Batch 29000: Loss 0.1551\n",
      "Batch 29100: Loss 0.1551\n",
      "Batch 29200: Loss 0.1551\n",
      "Batch 29300: Loss 0.1551\n",
      "Batch 29400: Loss 0.1551\n",
      "Batch 29500: Loss 0.1551\n",
      "Batch 29600: Loss 0.1550\n",
      "Batch 29700: Loss 0.1551\n",
      "Batch 29800: Loss 0.1551\n",
      "Batch 29900: Loss 0.1552\n",
      "Batch 30000: Loss 0.1552\n",
      "Batch 30100: Loss 0.1552\n",
      "Batch 30200: Loss 0.1551\n",
      "Batch 30300: Loss 0.1551\n",
      "Batch 30400: Loss 0.1551\n",
      "Batch 30500: Loss 0.1551\n",
      "Batch 30600: Loss 0.1551\n",
      "Batch 30700: Loss 0.1550\n",
      "Batch 30800: Loss 0.1551\n",
      "Batch 30900: Loss 0.1551\n",
      "Batch 31000: Loss 0.1551\n",
      "Batch 31100: Loss 0.1551\n",
      "Batch 31200: Loss 0.1551\n",
      "Batch 31300: Loss 0.1551\n",
      "Batch 31400: Loss 0.1551\n",
      "Batch 31500: Loss 0.1551\n",
      "Batch 31600: Loss 0.1551\n",
      "Batch 31700: Loss 0.1551\n",
      "Batch 31800: Loss 0.1551\n",
      "Batch 31900: Loss 0.1551\n",
      "Batch 32000: Loss 0.1551\n",
      "Batch 32100: Loss 0.1551\n",
      "Batch 32200: Loss 0.1550\n",
      "Batch 32300: Loss 0.1550\n",
      "Batch 32400: Loss 0.1550\n",
      "Batch 32500: Loss 0.1551\n",
      "Batch 32600: Loss 0.1551\n",
      "Batch 32700: Loss 0.1551\n",
      "Batch 32800: Loss 0.1551\n",
      "Batch 32900: Loss 0.1551\n",
      "Batch 33000: Loss 0.1551\n",
      "Batch 33100: Loss 0.1551\n",
      "Batch 33200: Loss 0.1551\n",
      "Batch 33300: Loss 0.1551\n",
      "Batch 33400: Loss 0.1551\n",
      "Batch 33500: Loss 0.1551\n",
      "Batch 33600: Loss 0.1551\n",
      "Batch 33700: Loss 0.1551\n",
      "Batch 33800: Loss 0.1551\n",
      "Batch 33900: Loss 0.1551\n",
      "Batch 34000: Loss 0.1551\n",
      "Batch 34100: Loss 0.1551\n",
      "Batch 34200: Loss 0.1551\n",
      "Batch 34300: Loss 0.1551\n",
      "Batch 34400: Loss 0.1550\n",
      "Batch 34500: Loss 0.1551\n",
      "Batch 34600: Loss 0.1551\n",
      "Batch 34700: Loss 0.1551\n",
      "Batch 34800: Loss 0.1551\n",
      "Batch 34900: Loss 0.1551\n",
      "Batch 35000: Loss 0.1552\n",
      "Batch 35100: Loss 0.1551\n",
      "Batch 35200: Loss 0.1551\n",
      "Batch 35300: Loss 0.1551\n",
      "Batch 35400: Loss 0.1551\n",
      "Batch 35500: Loss 0.1552\n",
      "Batch 35600: Loss 0.1552\n",
      "Batch 35700: Loss 0.1552\n",
      "Batch 35800: Loss 0.1551\n",
      "Batch 35900: Loss 0.1551\n",
      "Batch 36000: Loss 0.1552\n",
      "Batch 36100: Loss 0.1552\n",
      "Batch 36200: Loss 0.1552\n",
      "Batch 36300: Loss 0.1552\n",
      "Batch 36400: Loss 0.1551\n",
      "Batch 36500: Loss 0.1551\n",
      "\n",
      "Epoch 2 Metrics:\n",
      "AUC: 0.7484 | MRR: 1.0000 | NDCG@5: 0.0406 | NDCG@10: 0.0406\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "__main__.NewsDataset = NewsDataset  # Critical for Jupyter\n",
    "\n",
    "device = 'cpu'\n",
    "model = NRMS().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create dataset and loader\n",
    "dataset = NewsDataset(random.sample(samples, int(len(samples)*0.2)), news_dict)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "print(len(tqdm(loader)))\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(tqdm(loader)):\n",
    "        history = batch['history'].to(device)\n",
    "        candidate = batch['candidate'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(history, candidate)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i}: Loss {total_loss/(i+1):.4f}\")\n",
    "    \n",
    "    auc, mrr, ndcg5, ndcg10 = evaluate(model, loader)\n",
    "    print(f\"\\nEpoch {epoch+1} Metrics:\")\n",
    "    print(f\"AUC: {auc:.4f} | MRR: {mrr:.4f} | NDCG@5: {ndcg5:.4f} | NDCG@10: {ndcg10:.4f}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f78f88af-ea83-4a0b-ba84-75a8cda13e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss/len(loader),\n",
    "}, 'nrms_model_checkpoint.pth')\n",
    "\n",
    "# Additionally save just the model weights (smaller file)\n",
    "torch.save(model.state_dict(), 'nrms_model_weights.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "289b19fb-393c-4631-95d3-0015c585720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "\n",
    "MAX_LEN = 64\n",
    "\n",
    "def load_model(checkpoint_path='nrms_model_checkpoint.pth'):\n",
    "    # Initialize model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = NRMS()\n",
    "    \n",
    "    # Load saved state\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e460f0-8f5b-4a7e-a2bd-3fde4c0bf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import load_model\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load_model('nrms_model_checkpoint.pth')\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Example usage\n",
    "def predict(history_news_ids, candidate_news_id):\n",
    "    # Tokenize input\n",
    "    history_tokens = [news_dict.get(nid, [0]*MAX_LEN) for nid in history_news_ids]\n",
    "    candidate_tokens = news_dict.get(candidate_news_id, [0]*MAX_LEN)\n",
    "    \n",
    "    # Pad/truncate history\n",
    "    if len(history_tokens) > 50:\n",
    "        history_tokens = history_tokens[-50:]\n",
    "    else:\n",
    "        history_tokens.extend([[0]*MAX_LEN]*(50 - len(history_tokens)))\n",
    "    \n",
    "    # Convert to tensors\n",
    "    history_tensor = torch.tensor([history_tokens])\n",
    "    candidate_tensor = torch.tensor([candidate_tokens])\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        score = model(history_tensor, candidate_tensor).item()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "231cfc90-8bba-44dd-9c6f-0c37ac8e1824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 156964 samples (100% of full dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 156964/156964 [1:47:56<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Metrics (100% subset):\n",
      "AUC: 0.6825 | MRR: 0.3923 | NDCG@5: 0.3686 | NDCG@10: 0.4250\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def compute_dcg(relevances, k):\n",
    "    relevances = np.array(relevances)[:k]\n",
    "    if len(relevances) > 0:\n",
    "        return np.sum(relevances / np.log2(np.arange(2, len(relevances)+2)))\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_nrms(model, behaviors_df, news_dict, show_progress=True):\n",
    "    auc_scores, mrr_scores, ndcg5_scores, ndcg10_scores = [], [], [], []\n",
    "    \n",
    "    # Create progress bar if requested\n",
    "    iterator = behaviors_df.iterrows()\n",
    "    if show_progress:\n",
    "        iterator = tqdm(iterator, total=len(behaviors_df), desc=\"Evaluating\")\n",
    "    \n",
    "    for _, row in iterator:\n",
    "        # Parse impressions\n",
    "        impressions = []\n",
    "        for item in row['Impressions'].strip().split():\n",
    "            parts = item.split('-')\n",
    "            if len(parts) == 2:\n",
    "                impressions.append((parts[0], int(parts[1])))\n",
    "        \n",
    "        if not impressions:\n",
    "            continue\n",
    "            \n",
    "        news_ids, labels = zip(*impressions)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            scores = []\n",
    "            for nid in news_ids:\n",
    "                # Create dummy history if empty\n",
    "                history = row['History'].split() if pd.notna(row['History']) else [nid]\n",
    "                \n",
    "                # Prepare input tensors\n",
    "                history_tokens = [news_dict[nid] for nid in history if nid in news_dict]\n",
    "                if not history_tokens:\n",
    "                    history_tokens = [[0]*MAX_LEN]\n",
    "                    \n",
    "                candidate_tokens = news_dict.get(nid, [0]*MAX_LEN)\n",
    "                \n",
    "                # Pad/truncate history\n",
    "                if len(history_tokens) > 50:\n",
    "                    history_tokens = history_tokens[-50:]\n",
    "                else:\n",
    "                    history_tokens.extend([[0]*MAX_LEN]*(50 - len(history_tokens)))\n",
    "                \n",
    "                # Convert to tensors\n",
    "                history_tensor = torch.tensor([history_tokens]).to(device)\n",
    "                candidate_tensor = torch.tensor([candidate_tokens]).to(device)\n",
    "                \n",
    "                # Get prediction\n",
    "                score = model(history_tensor, candidate_tensor).item()\n",
    "                scores.append(score)\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if 0 < sum(labels) < len(labels):\n",
    "            auc_scores.append(roc_auc_score(labels, scores))\n",
    "        \n",
    "        order = np.argsort(-scores)\n",
    "        \n",
    "        # MRR\n",
    "        for rank, idx in enumerate(order):\n",
    "            if labels[idx] == 1:\n",
    "                mrr_scores.append(1.0/(rank+1))\n",
    "                break\n",
    "        else:\n",
    "            mrr_scores.append(0.0)\n",
    "        \n",
    "        # NDCG\n",
    "        sorted_labels = labels[order]\n",
    "        ideal_labels = sorted(labels, reverse=True)\n",
    "        \n",
    "        dcg5 = compute_dcg(sorted_labels, 5)\n",
    "        dcg10 = compute_dcg(sorted_labels, 10)\n",
    "        idcg5 = compute_dcg(ideal_labels, 5)\n",
    "        idcg10 = compute_dcg(ideal_labels, 10)\n",
    "        \n",
    "        ndcg5_scores.append(dcg5/idcg5 if idcg5 > 0 else 0.0)\n",
    "        ndcg10_scores.append(dcg10/idcg10 if idcg10 > 0 else 0.0)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    metrics = (\n",
    "        np.mean(auc_scores) if auc_scores else 0,\n",
    "        np.mean(mrr_scores) if mrr_scores else 0,\n",
    "        np.mean(ndcg5_scores) if ndcg5_scores else 0,\n",
    "        np.mean(ndcg10_scores) if ndcg10_scores else 0\n",
    "    )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Load the model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Sample a tenth of the dataset for evaluation\n",
    "evaluation_subset = behaviors_df.sample(frac=1, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "print(f\"Evaluating on {len(evaluation_subset)} samples (100% of full dataset)\")\n",
    "nrms_metrics = evaluate_nrms(model, evaluation_subset, news_dict)\n",
    "\n",
    "print(\"\\nFinal Evaluation Metrics (100% subset):\")\n",
    "print(f\"AUC: {nrms_metrics[0]:.4f} | MRR: {nrms_metrics[1]:.4f} | NDCG@5: {nrms_metrics[2]:.4f} | NDCG@10: {nrms_metrics[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124375ec-1350-4594-8338-3a61e551b89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env]",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
